{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Project Code for ST446 Final Project\n","***By Aleksander Brynjulf Hubert***\n","\n","***May 2020***\n","\n","\n","This code is used for my experiment comparing dimensionality reduction of word vectors. This code was run on a GCP machine and was used to process all of the word vectors to then be tested using QVEC. This file contains the code that cleans, tokenizes, and builds the word vector using the spark implementation of word2vec. The final chunks take the word vectors and use a Singular Value Decomposition to reduce the dimensionality from 150 to 75, 50, 25, and 10 dimensions. Note where it says '{BUCKET_NAME}' this will have to be replaced with the name of your bucket. For more information see the writeup."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import re\n","import pyspark\n","from pyspark import SparkContext\n","from pyspark import SparkConf\n","import itertools\n","import collections\n","import nltk\n","\n","from pyspark.mllib.feature import Word2Vec\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","import string\n","\n","from pyspark.mllib.linalg import DenseMatrix\n","from pyspark.mllib.linalg.distributed import RowMatrix"]},{"cell_type":"markdown","metadata":{},"source":["## Data Cleaning Functions\n","These functions are set up to clean and tokenize the data using NLTK and removing English stopwords. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stop_words = set(stopwords.words('english'))\n","table = str.maketrans('', '', string.punctuation)\n","\n","def get_tokens(line):\n","    tokens = word_tokenize(line)\n","    tokens = [w.lower() for w in tokens]\n","    stripped = [w.translate(table) for w in tokens]\n","    words = [word for word in stripped if word.isalpha()]\n","    return (words)\n","\n","def get_page(content):\n","    #Remove any leading or lagging space if present \n","    #content = content.encode('utf8').strip()\n","    page = []\n","    try:\n","        if(content != [] and content != None):\n","            # Split the content on the basis of new line\n","            page = content.split(\"\\n\", 2)\n","            page[2] = get_tokens(page[2])\n","    except:\n","        page = [1,2,3]\n","    return page\n"]},{"cell_type":"markdown","metadata":{},"source":["## Functions for Handling Word Vectors\n","This chunk sets up the helper functions for running the project. This includes a utility function which changes a list to a string separated by a whitespace. A function for creating an RDD to hold the wordvectors. A function that reduces dimensionality and finally a function that outputs the word vectors into the correct format as a txt. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def list_to_srting(lst): \n","    return ' '.join(map(str, lst)) \n","\n","##This function formats the word vectors from word2vec.model.getVectors()\n","def format_wordvectors(vectors):\n","    #array for storing the actual vectors\n","    vec_nums = [] \n","    #array for storing the words in order\n","    words = [] \n","    #iterates over the vectors\n","    for k in vectors:\n","        words.append(k)\n","        vec_nums.append(list(vectors[k]))\n","\n","    #creates and RDD of the vectors where each row represents a vector\n","    vec_rdd = sc.parallelize(vec_nums)\n","    return vec_rdd, words\n","\n","##This function reduces the dimension of the vector RDD using SVD from pyspark.mllib\n","def dim_reduction(vec_rdd, words, k):\n","    #creates a RowMatrix from pyspark.mllib.linalg\n","    mat = RowMatrix(vec_rdd)\n","    #computes the SVD with the given leading singular values K\n","    vectors_dimReduce = mat.computeSVD(k, computeU=True)\n","\n","    #Creates a dense matrix S for multiplication with matrix U\n","    s_dense = DenseMatrix(len(vectors_dimReduce.s), len(vectors_dimReduce.s),np.diag(vectors_dimReduce.s).ravel(\"F\"))\n","    #multiplies U*S\n","    dim_reduce_list = vectors_dimReduce.U.multiply(s_dense).rows.collect() \n","\n","    #Creates a dictionary where the key is the word and the value is the word vector\n","    svd_dictionary = {}\n","\n","    for i in range(0,len(words)):\n","        svd_dictionary[words[i]] = dim_reduce_list[i]\n","    return svd_dictionary\n","\n","##This function outputs the word vectors in the desired format for SVD\n","## and saves them as a .txt in the bucket\n","import datalab.storage as gcs\n","def doc_output(vec_dict, bucket, filename, output_folder):\n","    final_doc_dimReduce = ''\n","    for k in vectors:\n","        final_doc_dimReduce = final_doc_dimReduce + k + ' ' + list_to_srting(list(vec_dict[k])) + '\\n'\n","    gcs.Bucket(bucket).item(filename).write_to(final_doc_dimReduce, output_folder)"]},{"cell_type":"markdown","metadata":{},"source":["## Experiment\n","This next section uses the DBLP bibliography to test the code. Due to the limitations of compute credits I chose to use the DBLP data since it was smaller and would not use up all the credits during testing. The instructions to set up the environment are in the appendix of the writeup.\n"]},{"source":["### File Reading\n","This next chunk reads the files in from the data folder in the bucket. Then it extracts and tokenizes the title of each paper from the DBLP database and stores them in an RDD."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["##Read in the author data\n","author_file = sc.\\\n","    textFile(\n","        \"gs://{BUCKET_NAME}/data/author.txt\", \n","        4)\n","\n","#Extracts and creates and RDD of the tokenized titles\n","def extract_titles(row):\n","    row = row.strip()\n","    row_split = np.array(row.split(\"\\t\"))\n","    title_text = get_tokens(row_split[2])\n","    return title_text\n","\n","author_rdd = author_file.map(extract_titles)\n","author_rdd.take(5)"]},{"cell_type":"markdown","metadata":{},"source":["### Word2Vec Vectors\n","This chunk trains the word2vec model using pyspark."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["##runs the word2vec model with 100 dimensions\n","word2vec = Word2Vec().setSeed(42)\n","model = word2vec.fit(author_rdd)\n","vectors = model.getVectors()"]},{"cell_type":"markdown","metadata":{},"source":["#### File Output For Reduced Word Vectors\n","This chunk sets up the output of the wordvectors into the 'vectors/' folder within the assigned bucket."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["doc_output(vectors, '{BUCKET_NAME}', 'vectors/baseline.txt', 'vectors/')"]},{"cell_type":"markdown","metadata":{},"source":["### Dimensionality Reduction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#formats the wordvectors\n","formatted = format_wordvectors(vectors)\n","#separates words from their vectors for dimensionality reduction\n","vec_rdd = formatted[0]\n","word = formatted[1]\n","vec_rdd.take(20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#array of dimensions to test\n","dimensions = [75,50,25,10]\n","#array to store the word vectors \n","dim_models = []\n","#runs dimensionaility reduction for dimensions in the dimension array\n","for i in dimensions:\n","    dim_models.append(dim_reduction(vec_rdd, word, i))\n","    "]},{"cell_type":"markdown","metadata":{},"source":["#### File Output For Reduced Word Vectors\n","Finally this chunk output all the wordvectors into 'txt' files that are formatted for usage with the QVEC software."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#array to store the output filenames\n","output_filenames = ['vectors/k75.txt','vectors/k50.txt','vectors/k25.txt','vectors/k10.txt']\n","\n","#outputs the vectors to test with QVEC\n","for i in range(0, len(output_filenames)):\n","    doc_output(dim_models[i], '{BUCKET_NAME}', output_filenames[i], 'vectors/')\n"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":2}